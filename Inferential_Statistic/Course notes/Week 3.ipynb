{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up a test for a population proportion\n",
    "\n",
    "## Hypothesis Testing\n",
    "**Why do we do Hypothesis tests?** <br>\n",
    "Could the value of the parameter be maru maru? <br>\n",
    "we have an idea of what it could be but we have no idea if it's correct or not. <br>\n",
    "with that question we try and collect data to support that claim or go against that claim. <br>\n",
    "\n",
    "## C.S. Mott Children't hospital\n",
    "\n",
    "## Research Question:\n",
    "In previous years, 52% of parents believed that electronics and social media was the cause of their teenager's lack of sleep. Do more parents today believe that their teenager's lack of sleep is caused due to electronics and social media?\n",
    "<br>\n",
    "Population: Parents with teenages aged 13-18 <br>\n",
    "Parameter of interest = p or the population proportion <br>\n",
    "**Test for a significance increase in the proportion of parents with a teenager who believe that electronics and social media is the cause for lack of sleep.** <br>\n",
    "\n",
    "### We set our hypothesis first, even before we collect our data so that we do not influence in what we believe.\n",
    "\n",
    "H0 : Null Hypothesis : p = 0.52 -> Lack of sleep is caused by electronics<br>\n",
    "Ha : Alternate hypothesis : p ? 0.52 (? in [>, <. !=]) <br>\n",
    "In our case here Ha : p > 0.52 as the statement above says check for a significance increase. <br>\n",
    "**p here is the population proportion of parents with a teenager who believe that electronics and social media is the cause of their teenager's lack of sleep** <br>\n",
    "alpha = 0.05 -> Significance level : When we find something to be significant when p is < 0.05 : cutoff point <br>\n",
    "After this we will collect the data.\n",
    "### Assumptions:\n",
    "1. Random sample of parents\n",
    "2. Large enough sample size \" n*p0 and n*(1-p0) > 10\n",
    "\n",
    "<br>\n",
    "From the poll after collection data, we got: <br>\n",
    "**Best estimate of p is p^ = 0.56** <br>\n",
    "\n",
    "\n",
    "### Test statistic:\n",
    "**(Best estimate - Hypothesized estimate) / Standard Error of estimate** <br>\n",
    "**(p^ - po) / s.e** <br>\n",
    "se(p^) = sqrt( (p . (1-p)) / n ) <br>\n",
    "since we do not know what p is, <br>\n",
    "**se(p^) = sqrt( (p . (1-p)) / n ) -> Null Standard Error**<br> \n",
    "Since we **do not know** what **p** is, we will use **p0 to calculate our se(p^)** <br>\n",
    "**se(p^) = sqrt( (p0. (1-p0)) / n ) \n",
    "<br>\n",
    "Finally, <br>\n",
    "**Z(test statistic) = (p^ - p0) / null se(p^)**\n",
    "<br><br>\n",
    "Z statistic means that our observed sample proportion is **(value of Z)** null standard errors above our hypothesized population. <br>\n",
    "Here, <br>\n",
    "Z = (p^ - p0) / s.e <br>\n",
    "se = sqrt( (p0 . (1-p0)) / n ) <br>\n",
    "se = 0.0157 <br>\n",
    "Z = (0.56 - 0.52) / 0.0157 <br>\n",
    "Z = 2.555 <br>\n",
    "We will consider this a normal distribution as we are calculating a proportion (population statistic) and our sample size is large enough. <br>\n",
    "**Test Statistic Interpretation** <br>\n",
    "Z = 2.555 <br>\n",
    "That means that our observed sample proportion is 2.555 null standard errors above our hypothesized population proportion. <br>\n",
    "\n",
    "### Test Statistic Distribution\n",
    "* A Z test statistic is another random variable! It has a distribution.\n",
    "* The Z test statistic will always follow a N(0,1), i.e. Mean = 0 and std-dev = 1 \n",
    "* This is due to us centering and scaling our original data.\n",
    "    * Z = (p^ - p0) / se(p^)\n",
    "        * p^-p0 centers our data\n",
    "        * se(p^) scales our data\n",
    "\n",
    "### P Value\n",
    "![p-value](img/p-value-single-prop.png)\n",
    "<br><br>\n",
    "**We get the P-value from a Z-table or by calculating it from some programming language. Here, p-value = 0.0053 which means that it is less than our significance level, alpha, so our null hypothesis sounds rediculous, and we will reject it.** <br>\n",
    "p-value = 0.0053 < alpha =0.05  <br>\n",
    "Reject the Null Hypothesis (H0:p = 0.52) <br>\n",
    "**There is sufficient evidence to conclude that the population proportion of parents with a teenager who believe that electronics and social media is the cause for lack of sleep is greater than 52% (our alternate hypothesis)** <br>\n",
    "**We would fail to reject a Null Hypothesis when the p-value is > alpha.** <br>\n",
    "\n",
    "### Summary\n",
    "* 4 main steps to a hypothesis test\n",
    "    - stating a  hypothesis and selecting a significance level (alpha)\n",
    "    - Checking assumptions\n",
    "    - Calculating a test statistic and gettinga  p-value from the test statistic\n",
    "    - drawing conclusions from the p-value\n",
    "* The Z test statistic distribution is N(0,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up a test of difference in population proportions\n",
    "**Research Question**\n",
    "Is there a significance difference between the population proportions of parents of black children and parents of hispanic children who report that their child has some swimming lessons?  <br>\n",
    "**Populations**  <br>\n",
    "1. All parents of black children aged 6-18 (group - 1)\n",
    "2. All parents of hispanic children aged 6-18 (group - 2)\n",
    "**Parameter of interest** <br>\n",
    "p1 - p2 <br>\n",
    "**Test for a significant difference in the population proportions of parents reporting their child has had swimming lessons at the to% significance level.** <br>\n",
    "H0: p1-p2 = 0 <br>\n",
    "**Null Hypothesis always has to have the equal sign**\n",
    "ha: p1-p2 ? 0  <br>\n",
    "**In this case, alternate will be != as we just want to see if the two population proportions are unequal. we do not care what direction the inequality is in**\n",
    "Thus, <br>\n",
    "Ha: p1-p2 != 0  <br>\n",
    "alpha = 0.10 <br>\n",
    "\n",
    "## Assumptions\n",
    "1. Two independent random samples\n",
    "2. Large enough sample size, i.e. n1p^, p1(1-p^), n2p^, n2(1-p^) >= 10\n",
    "\n",
    "## Checking Assumptions\n",
    "p^ = (91+120)/(247+308) = 211/555 = 0.38 <br>\n",
    "247(0.38) = 94 <br>\n",
    "247(1-0.38) = 153 <br>\n",
    "308(0.38) = 117 <br>\n",
    "308(1-0.38) = 191 <br>\n",
    "**If this assumption is not met, we can perform different tests that bypass this assumption** <br>\n",
    "**p1^ = 91/247 = 0.37, 1 = black <br>\n",
    "p2^ = 120/308 = 0.39, 2 = hispanic** <br>\n",
    "**p1^ - p2^ = 0.37 - 0.39 = -0.02 <br>\n",
    "Here, as the difference is negative, we see that the sample proportion for black children is smaller than the sample proportion of hispanic childten. <br>\n",
    "\n",
    "## Testing Difference in population Proportions\n",
    "\n",
    "## Test Statistic\n",
    "**(Best estimate - Hypothesized estimate) / Standard error of estimate**\n",
    "<br>\n",
    "(p1^ - p2^ -0) / se(p^) <br>\n",
    "se(p^) = sqrt( p^(1-p^) . { (1/n1) + (1/n2) } )\n",
    "<br>\n",
    "sq(p^) = 0.041 <br>\n",
    "p1^-p2^-0 = -0.02 <br>\n",
    "Z = -0.02/0.041 <br>\n",
    "Z = -0.48 <br>\n",
    "\n",
    "### Test Statistic Interpretation\n",
    "Z = -0.48 <br>\n",
    "That means that our observed difference in sample proportions is 0.48 estimated standard errors below our hypothesized mean of equal propulation proportions.  <br>\n",
    "\n",
    "### p-value:\n",
    "![p-value](img/p-value-double-prop.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision and conclusion\n",
    "p-value = 0.63 > 0.10 = alpha -> Fail to reject the null hypothesis <br>\n",
    "**This does not mean we accept the null hypothesis, it only means that we do not have enough evidence to reject it.** <br>\n",
    "* This means we don't have evidence against equal population proportions\n",
    " Formally, based on our sample and our p-value, we fail to reject the null hypothesis. We conclude that there is no significant difference between the population proportion of parents of black and hispanic children who report their child has had swimming lessons.  <br>\n",
    " **In essence, it means that the population proportion of both populations are roughly equivalent, meaning it's not like black or hispanic children have more/less swimming lessons that their counterparts.** <br>\n",
    " ## Alternative Approaches\n",
    " 1. CHhi-Square (x^2) Test\n",
    "     * Different Hypotheses\n",
    "     * Require two-sided hypothesis : Does not work for p1 > p2 or p1 < p2. Works for p1 != p2\n",
    "     * same conclusion* as population proportion test (one we did above)\n",
    "     * As two-sided hypothesis with proportions\n",
    "2. Fisher's Exact Test\n",
    "    * Allows one-sided hypothesis\n",
    "    * typocally small sample sizes\n",
    "    * Calculates different p-values compared to population proportion test\n",
    "    * Compared to same setup of proportions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# p-values, p-hacking, and so on\n",
    "p-hacking: or any other statistic hacking happens when the researcher intentionally/unintentionally manipulates the data such that it gives us a different result than it would otherwise give. For example, during data collection, if the sample isn;t good enough or if the outliers were removed during data cleaning or so on, it will result in a p-value/other statistic value that isn't really interpretable.  <br>\n",
    "For example, if we have a hypothesis and we are managing our data and see that it isn't really making much sense so we take a smaller sample of a section of that data and then find the statistic, this is a form of hacking. We get another result that isn't really correct.  <br>\n",
    "Another problem with this is that there isn't really any way to check it as most researchers just give information on how the final p value was calculated. They do not provide all the other methods used, all the other analysis done, all the other p-values calculated. So, it is really difficult to see if a statistic is hacked until someone else carries out the same study, sometimes years later and gets a different result. That makes a study very in-interpretable.  <br>\n",
    "Another way of p-hacking can be caused by asking the wrong questions. Like, instead of a > H0 for the alternate hypothesis on a one sided test, we choose a < H0 as our alternate hypothesis on the one-sided test.  <br>\n",
    "**P value is the measure of surprise. The lower it is, the more surprised we are at our data because of how we choose our Null Hypothesis. We will never be surprised at our hypothesis as we make those before we collect our data. We get surprised with a low p-value because the data does not fall in line with our default action, i.e. our Null Hypothesis**\n",
    " <br>\n",
    " \n",
    " **p_hacking refers to testing multiple observations on the same data until one of them is statistically significant and then publish or present those results as mathematically valid conclusion. THIS IS WRONG. Plan your test ahead, define relevant segmentation before chosing a p-value, don;t peek into your data.** <br>\n",
    " \n",
    " **Hypotheses are “guesses” about model and data structure that we want to test from sample data. Hypothesis Testing is statistical term for how to do so in mathematically meaningful way, but we do so all the time even in daily experience without realizing.** If TV isn’t working and you restarted the set-top-box, you have just tested a hypothesis. Hypothesis was that cause of TV not working lies in set-top-box, and restarting is way to test if that is correct. If your observations (result after restarting) align with your hypothesis (TV starts working or gets better), then you have more confidence in hypothesis. Then you may test another. **Problem solving is essentially serial hypothesis testing.** As we discussed in previous post on [“11 facts of data science“](https://www.edupristine.com/blog/11-facts-about-data-science), alternative to hypothesis testing is trial-and-error, where you will just tinker with everything and hope that something works.\n",
    "\n",
    "In statistical terms hypothesis testing refers to having a prior belief (called Null Hypothesis), observing data and doing certain calculations on it, and seeking strong enough evidence to falsify prior belief (reject Null Hypothesis). Recall that **in practice there is never certain, or 100%, evidence for anything** – even that Sun will rise tomorrow – but only strong enough, say, 99.9…9%. **There is slight risk inherent in rejecting null hypothesis without certainty, and that risk is represented as confidence level of α (alpha).** There is, naturally, alternate risk of being too stubborn and demanding too much evidence, that we stick to prior belief even in presence of extraordinary evidence otherwise. That risk is less talked about, and is represented as β (beta) or power of the test (1-β ). \n",
    "\n",
    "How small chance can you afford depends entirely on practical risk of making wrong decision. What if you claim someone has cancer when he doesn't? What about claiming he doesn't  when does? Which is more risky? What about declining valid credit card transaction thinking it is fraud? What about not declining and actually risking fraud? **Since α and β always play against each other, there is trade off in risk of false positive and false negative.** Depending on your application you may accept anywhere from 20% to 0.001% risk in rejecting null hypothesis falsely. If your application isn’t specific, or both errors are equally bad, then statistical rule of thumb has emerged for α=5%. That means, about 1 in 20 times you will reject null hypothesis falsely just because sample happens to be on right on black line.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Hypothesis about a population mean\n",
    "\n",
    "**Research Question** <br>\n",
    "Is the ___average___ cartwheel distance for adults more than 80 inches? <br>\n",
    "**Population:** All Adults <br>\n",
    "**parameter of Interest:** Population mean cartwheel distance mu <br>\n",
    "Perform a one-sample test regarding the value for the mean cartwheel distance for the population of all such adults. <br>\n",
    "**H0: mu = 80 <br>\n",
    "H1: mu > 80 <br>**\n",
    "Where mu represents the population mean cartwheel distance in inches for all adults. <br>\n",
    "**Significance level, alpha = 5%** <br>\n",
    "\n",
    "### How much evidence do we require to go against that Null hypothesis and in favour of the alternate theory. If results are more unusual under the null theory than we will see those results no more than 5% of the time. If results are more extreme/unusual than that 5%, we will reject the null hypothesis and go with the alternate hypothesis.\n",
    "\n",
    "**mean** = 82.48 <br>\n",
    "We see it's more than 80. But the question is, is it significantly more than 80? <br>\n",
    "**Std-dev** = 15.06 <br>\n",
    "## We perform a t-test\n",
    "\n",
    "![Cartwheel graphs](img/cartwheel-graphs.png)\n",
    "**Even tho there is deviation from normality, since we have a large enough sample, we will apply CLT and carry on**\n",
    "\n",
    "## Examining the results\n",
    "* Is sample mean 82.48 inches signigicantly larger than the hypothesized mean of 80 inches?\n",
    "* Calc std error: sigma / sqrt(n)\n",
    "    * we don;t know what sigma is, so use Estimated standard error of sample mean, s/sqrt(n)\n",
    "### T Statistic: Assuming sampling distribution of sample mean is normal\n",
    "t = (best estimate - null value) / estimated standard error <br>\n",
    "= sbar - 80 / (s/sqrt(n))  <br>\n",
    "= 82.48 - 80  / 15.06/sqrt(25) <br>\n",
    "= 0.82 <br>\n",
    "**Our sample mean is only 0.82 estimated standard errors above null value of 80 inches. Not evel 1 std err. That is not so different.**\n",
    "## Determine p-value\n",
    "* if null hypothesis was true, would a test statistic value of only t=0.82 be unusual enough to reject the null?\n",
    "* p-value = Probability of seeing test statistic of 0.82 or more extreme assuming the null hypothesis is true\n",
    "* If null hypothesis was true, t test statistic follows a Student t distribution with degrees of freedom n-1 = 24\n",
    "* Since we have a one tailed test to the right,\n",
    "    - More extreme values are measured to the right (upper tail)\n",
    "![p-value-calculation](img/determine-p-value.png)\n",
    "So, we see that if our H0 was really true, we would see a value that extreme 20% of the time. It isn't very unusual. <br>\n",
    "## Make a decision about Null\n",
    "Since our p-value is much bigger than 0.05 significance level, weak evidence against null \n",
    "* We **Fail to reject the Null Hypothesis** \n",
    "Based on estimated mean (82.48 inches), we **cannot support** the population mean CW distance is greater than 80 inches <br>\n",
    "### We try the same with confidence intervals. 90% confidence interval in this case\n",
    "mean = 82.48 inches <br>\n",
    "std dev = 15.08 inches <br>\n",
    "n = 25 -> t* = 1.711 <br>\n",
    "conf-interval = xbar +- t* . (s/sqrt(n)) <br> \n",
    "= 82.48 +- 1.711 * 15.08/5 <br>\n",
    "=  82.48 +- 5.15  <br>\n",
    "(77.33, 87.63) inches <br>\n",
    "**Since 80 inches is IN the confidence interval of reasonable values for population mean CW distance and the entire interval is not > than 80, 82.48 is in our range of reasonable values.**\n",
    "## What if normality doesn't hold?\n",
    "* Not convinced that CW distance follows a normal distribution in population?\n",
    "    - Non parametric test\n",
    "* Non-parametric analog of one sample t-test\n",
    "    - Wilcoxon signed rank test\n",
    "        - uses *median* to examine location of distribution of measurements instead of median/\n",
    "        - we will get our p value here and then carry on like we did here\n",
    "#### Wilcoxon Signed rank test result: p-value >> 0.05\n",
    "Fail to reject null that population median CW distance is > 80inches <br>\n",
    "**Conclusion is robust to potential violations of normality!** <br> <br>\n",
    "For the population of interest of all adults, regardless of assumptions made and inference approach used, **There is Not sufficient evidence to support that the population mean CW distance is more than 80 inches** <br> <br>\n",
    "\n",
    "![Summary](img/summary.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing a population mean difference \n",
    "**Looking a paired data**\n",
    "In what way is our data paired? - based on the individual <br>\n",
    "**Research Question** <br>\n",
    "Is there an average difference  between the cabinat quotes from the suppliers? <br>\n",
    "**Populations:** All Houses <br>\n",
    "**Parameter of Interest:** Population mean difference of cabinet quotes mu_d (Supplier A - Supplier B) <br>\n",
    "## Hypothesis\n",
    "H0: mu_d = 0 <br>\n",
    "Ha: mu_d != 0 <br>\n",
    "alpha = 0.05 <br>\n",
    "## Assumptions\n",
    "1. Simple random sample of differences, i.e. Random sample of houses.\n",
    "2. **Population of differences to be normally distributed**\n",
    "    * sample size >= 25\n",
    "    * Here we have only 20 samples.\n",
    "    * **But since after the QQ plot, the points fall reasonably along the line, we can assume the population of differences are normally distributed**\n",
    "\n",
    "### Summerizing the data\n",
    "n = 20 observations <br>\n",
    "min = -30 dollars <br>\n",
    "max = 90 dollars <br>\n",
    "Median = 13.50 dollars <br>\n",
    "mean = 17.30 dollars <br>\n",
    "Std-dev = 28.49 dollars <br>\n",
    "### Test statistic\n",
    "Assuming the sampling dist of the sample mean difference is normal, <br>\n",
    "t = (Best estimate - hypothesized estimate) / estimated std error of estimate <br>\n",
    "t = (xbar_d - 0) / (s_d / sqrt(n)) <br>\n",
    "t = 17.30 / (28.49 / sqrt(20) ) <br>\n",
    "t = 17.30 / 6.37 <br>\n",
    "t = 2.72 <br>\n",
    "**t statistic = 2.72 means that our observed mean difference is 2.72 estimated standard errors above our null value of 0** <br>\n",
    "t(19) = t(20 degrees of freedom - 1) <br>\n",
    "![p-val-mean-cabinet-difference](img/p-val-mean-cabinet-difference.png)\n",
    "p-val = 0.014 means that we will observe a test statistic as extreme or more extreme only 1.4% of the time. <br>\n",
    "p-val = 0.014 < 0.05 = alpha -> Reject Null Hypothesis <br>\n",
    "-> Have evidence against mean difference cabinet quotes is 0 <br>\n",
    "Formally, based on our sample and our p-value, we reject the null hypothesis. We conclude that the mean difference of acbinet quote prices for suppliers A less B is **Significantly different** from 0. <br>\n",
    "### Calculate 95% confidence interval \n",
    "mean = 17.30 dollars <br>\n",
    "Std-dev, s_d = 28.49 dollars <br>\n",
    "n = 20 -> t* = 2.093 <br>\n",
    "Z = x_bar +- t* (s_d / sqrt(n))  <br>\n",
    "(3.97, 30.63)  <br>\n",
    "**We see that 0 is *NOT* in our range of reasonable values for mean differences in cabinet prices, so the mean difference is not going to be zero. In addition, since all the values are positive, we see that Supplier A is probably more expensive than Supplier B** <br>\n",
    "### Wilcoxon Signed Rank Test\n",
    "If normalty doesn't hold, we can use the Wilcoxon Signed Rank Test to test for the Median instead of the mean. <br>\n",
    "p-val = 0.020 <br>\n",
    "**Again, we reject the H0 and conclude that the median difference in the cabinet quotes of supplier A less B, is different from 0.** <br>\n",
    " <br> <br>\n",
    " ![pop-mean-diff](img/pop-mean-diff-cabinet.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing for a difference in population means (For independnt groups)\n",
    "**Research Question** <br>\n",
    "Considering Mexican American Adults (aged 18-29) living in the united states, do males have a significantly higher mean body mass index than females? <br>\n",
    "**Population:** All Mexican American adults aged 18-29 <br>\n",
    "**Parameter of interest (mu1 - mu2):** Body mass index or BMI (kg/m^2) <br>\n",
    "**Task:** Perform independent samples t-test regarding the value for the difference in mean BMI between males and females. <br>\n",
    "## Steps for Hypothesis Testing\n",
    "1. Define null and Alternate Hypotheses\n",
    "2. Examine data, check assumptions, and calculate test statistic\n",
    "3. Determine corosponding p-value\n",
    "4. make a decision about null hypothesis\n",
    " <br>\n",
    " Assume that null is true until we can provide enough statistical evidence.\n",
    "  <br>\n",
    "  #### Step 1: Define Hypothesis\n",
    "  **Null** There is no significant difference in mean BMI  <br>\n",
    "  **Alternative**: There is a significant difference in mean BMI <br>\n",
    "  (Both statements are for the specified populations <br>\n",
    "  H0: mu1 = mu2 or mu1 - mu2 = 0 <br>\n",
    "  H1: mu1 != mu2 or mu1 - mu2 != 0<br>\n",
    "  alpha = 0.05 <br>\n",
    "  #### Step 2: Examine the data\n",
    "  Mean: male: 23.57, female: 22.83 <br>\n",
    "  Std-dev = male:6.24, female: 6.43 <br>\n",
    "  n = male: 258, female: 239 <br>\n",
    "  **Assumption check**\n",
    "  * Sanoke random samples\n",
    "  * independent from one another\n",
    "  * Both populations of responses are approximately normal or sample sizes are large enough\n",
    "      - We see from the histograms and qq plots for both samples that they are not exactly normally distributed but since we have such a huge sample size, we will use CLT and so, normality assumption is not crucial.\n",
    " <br>\n",
    "**Calculate test statistic** <br> <br>\n",
    "best estimate = x1bar - x2bar = 23.57 - 22.83 = 0.74 <br>\n",
    "Is our sample mean difference of 0.74 significantly different than 0? <br>\n",
    "We will use a test statistic to find that. <br> <br> <br>\n",
    "**Test Statistic** <br>\n",
    "A measure of how far our sample statistic is from our hypothesized population parameter in terms of estimated standard errors. <br>\n",
    "The further away oir sample statistic is, the less confident we will be in our null hypothesized value. <br>\n",
    "**Calculating test statistic** <br>\n",
    "t = (best estimate - null value) / estimated standard error <br>\n",
    "For estimated standard errors, we can use 2 approaches. <br>\n",
    "**Pooled approach: The variance of the two populations are assumed to be equal. (sigma1^2 = sigma2^2) <br>\n",
    "Unpooled approach: The assumption of equal variances is dropped. **<br>\n",
    "**Pooled approach**: t = (x1bar - x2bar - H0) / sp . sqrt( (1/n1) + (1/n2) ) <br>\n",
    "t = (x1bar - x2bar - H0) / ( sqrt( ( (n1-1)s1^2 + (n2-1)s2^2 )  / (n1+n2-2) ) . sqrt( (1/n1) + (1/n2) ) ) <br>\n",
    "**Unpooled approach**: t = (x1bar - x2bar - H0) / sqrt( (s1^2/n1) + (s2^2/n2) )  <br>\n",
    " <br>\n",
    " In our data, we see that the std-dev of male and females on BMI is very similar, only 0.2 off. And Because the IQR's and standard deviations are similar, the pooled approach will be used. <br>\n",
    " t = 0.74 / (0.0898 * 6.332)  <br>\n",
    " t = 1.30 <br>\n",
    " **This tells us that our difference in sample means is only 1.30 estimated standard errors above the null difference of 0 kg/m^2**\n",
    " #### Step 3: Determine P value\n",
    " If the null hypothesis mu1-mu2 = 0 were true, would a test statistic value of 1.30 be unusual enough to reject the null?\n",
    " <br>\n",
    " **p-value: Assuming the null hypothesis is true, it is the probability of observing a test statistic of 1.30 or more extreme.** <br>\n",
    " Using a t(df) distribution where df = n1+n2-2 <br>\n",
    " Our alternate hypothesis is two-sided (mu1-mu2 != 0), So we will check both the upper and lower tail. <br>\n",
    " \n",
    " ![determine p-value](img/p-val-male-female.png)\n",
    "\n",
    "<br>\n",
    "**p-value = 0.19** <br>\n",
    "If the difference in population mean BMI between males and females was really 0, then observing a difference in sample means of 0.74 i.e. a t-statistic of 1.30 or more extreme is farely likely, almost 20%.<br>\n",
    "#### Make a decision\n",
    "Our p val is larger than the 0.05 significance level which means there is weak evidence against the null. <br>\n",
    "Thus, we **Fail to reject the nul!** <br>\n",
    "Based on our estimated difference in sample means, we can not support that there is significant difference between the population mean BMI for males and population mean BMI for females for the population of all  Mexican-Americans adults aged 18-29 living in the US.  <br>\n",
    "**95% confidence interval**\n",
    "Prevously we had calculated the confidence interval for the given data.  <br>\n",
    "it was <br>\n",
    "(-0.385, 1.865) <br>\n",
    "Our test value of 0 falls within our estimated confidence interval and is a reasonable value for the difference mean BMI.\n",
    " <br>\n",
    " \n",
    " \n",
    "![summary population mean difference for independent groups](img/summary-independent-group-pop-means.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothesis Testing: Other Considerations\n",
    "Statistical hypothesis testing reflects the scientific method, adapted to the setting of research involving data analysis. In this framework, a researcher makes a precise statement about the population of interest, then aims to falsify the statement. In statistical hypothesis testing, the statement in question is the null hypothesis. If we reject the null hypothesis, we have falsified it (to some degree of confidence). According to the scientific method, falsifying a hypothesis should require an overwhelming amount of evidence against it. If the data we observe are ambiguous, or are only weakly contradictory to the null hypothesis, we do not reject the null hypothesis.\n",
    "\n",
    "The framework of formal hypothesis testing defines two distinct types of errors. A type I error (false positive) occurs when the null hypothesis is true but is incorrectly rejected. A type II error occurs when the null hypothesis is not rejected when it actually is false. Most traditional methods for statistical inference aim to strictly control the probability of a type I error, usually at 5%. While we also wish to minimize the probability of a type II error, this is a secondary priority to controlling the type I error.\n",
    "\n",
    "All the standard statistical testing procedures perform well at controlling type I error under ideal conditions, but most of them can break down and give misleading results in practice. That is, if we claim to be conducting a test that has a 5% false positive rate, is this the actual false positive rate? As we discussed earlier in the setting of confidence intervals, several complications that can arise in practice will result in a statistical procedure not performing as intended. In order to reduce the risk of this happening, statisticians use statistical theory and computer simulations to assess the operating characteristics of testing procedures in various challenging settings.\n",
    "\n",
    "## Normality of the data\n",
    "One of the commonly-stated “assumptions” that is often raised as a caveat when presenting statistical findings is the issue of the data being normally distributed. While it is true that in some circumstances, strongly non-normal data can cause statistical tests to be misleading, in most settings, the data are not required to follow a normal distribution. In addition, there are other issues unrelated to normality that are potentially more likely to produce misleading results.\n",
    "\n",
    "Concerns about normality primarily center on the calibration of rejection regions for a test statistic, or equivalently, the manner in which p-values are computed. As with confidence intervals, the Z-score plays the central role. To be concrete, suppose we are conducting a test comparing two population means using independent samples (a “two sample t-test”). The corresponding sample means are x1_bar and x2_bar, and the test is based on the difference between them, which is x1_bar - x2_bar. This difference has a standard error, which we denote here by s (there are a few different ways to compute this standard error, s can refer to any of them here). The Z-score is (x1_bar - x2_bar) / s.\n",
    "\n",
    "Under the very strong assumption that the data are normally distributed, the Z-score follows a Student t-distribution, with degrees of freedom depending on the way that the standard error was constructed (in most cases the degrees of freedom will be between m-2 and m, where m is the combined sample size of the two samples being compared). If the data are not normally distributed, we can appeal to the central limit theorem (CLT), which states that the Z-score will be approximately normally distributed as long as the sample size is not too small.\n",
    "\n",
    "As discussed earlier in the setting of confidence intervals, there is no universal rule that states when the sample size is large enough to justify invoking the CLT. Rules of thumb between 20 and 50 are often stated. A smaller sample size is sufficient to invoke the CLT when the data approximately follow a normal distribution, and a larger sample size is needed if the data are strongly non-Gaussian.\n",
    "\n",
    "While normality is a consideration in some settings, it is mainly relevant when the sample size is small and the data are strongly non-Gaussian. Other issues can cause statistical tests to break down in a broader range of settings, so normality of the data should be seen as one of several factors that can impact the performance of a statistical test, and is often a relatively minor one at that.\n",
    "\n",
    "A useful approach in practice is to use the Student t-distribution to calculate rejection regions and p-values, even when the data are not expected to follow a normal distribution. Doing so is slightly conservative, in that the rejection region based on the t-distribution will be slightly smaller than the rejection region based on the normal distribution, and p-values based on the t-distribution will be slightly larger than p-values based on the normal distribution. As the sample size grows beyond around 50, there is little practical difference between using the t-distribution and the normal distribution when carrying out statistical hypothesis tests.\n",
    "\n",
    "## Clustering and data dependence\n",
    "One additional issue that can adversely impact the performance of a statistical test is the presence of unknown (or unmodeled) correlations or clustering in the data. For example, if the data values are observed in sequence (e.g. over time), with each value possibly being correlated with its neighbors, then we have “autocorrelation”, which is a form of dependence. Alternatively, we may have some form of grouping or clustering in the data. Methods for addressing these issues will be discussed in Course 3 of this specialization.\n",
    "\n",
    "## Causality\n",
    "Another issue to be aware of when conducting statistical tests is that of confounding and causality. This issue is especially relevant when interpreting the results of a statistical test. Suppose, for example, that two groups of people differ significantly in terms of some trait, e.g. people with fewer dental cavities are seen to have statistically lower risk of heart disease compared to people with more cavities. It is important to note that this effect could be due to a lurking factor, or to some form of selection bias. For example, the people with fewer cavities may be less likely to be smokers. Note that this is arguably not a problem with the statistical hypothesis test itself -- it may well be true that people with fewer cavities are less likely to have heart disease (in the sense of there being a real association between these two factors). Rather, it is an issue of what substantive conclusions may be drawn, especially when people draw a causal conclusion where one is not warranted.\n",
    "\n",
    "## Multiplicity\n",
    "A third pitfall that arises with statistical hypothesis testing, as well as with other forms of statistical inference such as confidence intervals, is that of “multiplicity”, which we will discuss next. Note that a variety of terms have been used to describe this issue, including “data dredging”, “multiple testing”, and “p hacking” (in reference to “p values”).\n",
    "\n",
    "Most statistical inference procedures, including both confidence intervals and hypothesis testing, are based on an idealized research design in which a single analysis with narrow scope is conducted using a data set. In practice, data analysis often involves data exploration coupled with formal inference. It is now widely accepted that doing this heedlessly can lead to misleading results, and in particular often leads to statistical evidence for research findings being overstated. This is a large topic; here we comment on a few aspects of it, starting with two examples of how multiplicity in data analysis can cause problems.\n",
    "\n",
    "* Suppose that a researcher habitually conducts a two-sample t-test comparing group means, then reports a confidence interval for the difference in population means only if the null hypothesis of the t-test is rejected. On one hand, this researcher is exhibiting good judgment, as it is often recommended to report an effect size along with the results of any hypothesis test (the point estimate of the population mean difference is an effect size in this setting). However, the confidence intervals selected in this way will have lower than the nominal coverage probability. This is an example of a broader issue sometimes called “selective inference”.\n",
    "* Suppose that there are natural ways to divide the population into subgroups. For example, imagine that a researcher is interested in whether people who sleep less than 7 hours per night on average during one month have greater gain of body weight in the following year. The researcher’s initial plan may have been to consider the general population, but perhaps the investigator then decides to carry out analyses separately in women and in men, in older people and in younger people, in smokers and in non-smokers, etc. Although it is possible that hypothesized effect may actually be much stronger in some of these subgroups than in others, repeatedly testing the same data on different subgroups will be likely to give rise to falsely positive evidence for an association. For example, if the researcher conducts 3 independent tests on the same data (e.g. on different subgroups of the data), with each test having a 5% false positive probability, then the probability that at least one of the tests will yield a false positive is over 14%.\n",
    "\n",
    "In recent years, researchers have identified more and more ways that multiple-testing can arise in practice, corrupting statistical findings. Almost always, multiple testing leads to overstatements of the confidence in findings, or to erroneous findings being reported. Fortunately, there are many approaches to remedying this issue. The easiest of these to apply is the Bonferroni correction, which essentially involves multiplying all p-values by the number of tests that were performed. For example, if we conduct 5 hypothesis tests, and one of them yields a p-value of 0.02, then we should adjust this p-value to 0.1 (= 0.02 * 5). Thus, this test which would have been deemed to be “statistically significant” if conducted in isolation will be not be seen as such following the Bonferroni adjustment.\n",
    "\n",
    "## Power\n",
    "A final consideration we will discuss here is statistical power. Power is often defined in a narrow sense as the probability of rejecting the null hypothesis when the null hypothesis is false. Loosely speaking, this is the probability of not making a type II error. More broadly, power can refer to any aspect of the study design or data analysis that would make it more likely for meaningful results to be attained. There is a branch of statistics focusing on formal “power analysis” that aims to develop concrete and quantitative ways to assess the statistical power in a given setting. We will not delve into these methods here, and instead will discuss at a higher level how low power intersects with and exacerbates some of the other complicating considerations discussed above.\n",
    "\n",
    "The type I error rate is controlled by the researcher (say at 5%), but this only represents the risk of drawing a false conclusion from a single test. In recent years, the notion of the “false discovery rate” (FDR) has been advanced to understand how often the conclusions drawn in a research process involving multiple formal inferential procedures are mistaken. Focusing on hypothesis tests, we can consider a situation where, for example, five tests are to be conducted. If two of the underlying null hypotheses are false, but the power to reject them is low (say it is only 20%), then around 25% of all the rejected null hypotheses were incorrectly rejected. This shows how the FDR is different than the type I error rate, which remains controlled here at 5%.\n",
    "\n",
    "People sometimes incorrectly believe that controlling the type I error rate at 5% means that there is only a 5% chance that any reported finding is wrong. As illustrated here, the probability that a reported finding is wrong can be much higher than the type I error rate. This “error inflation” is primarily driven by two factors -- a researcher who pursues hypotheses that are unlikely to be correct, and a researcher who carries out studies with low statistical power will both have higher FDR in their work overall. The latter issue is in principle addressable by encouraging researchers to pursue fewer, but higher quality studies (i.e. to pursue fewer studies with larger sample sizes rather than many studies with small sample sizes). The former issue is harder to address, but it reflects the fact that in some fields, especially difficult areas of science such as genetics and neuroscience, there is a poor fundamental understanding of the systems under study, which may lead to people speculating and pursuing hypotheses with weak theoretical grounding.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Relationship between Confidence Intervals & Hypothesis Testing\n",
    "Confidence intervals and hypothesis tests are both forms of formal statistical inference. There is a strong connection between them. When working with a single parameter, tests and intervals are essentially equivalent, but this equivalence is lost when there is more than one parameter.\n",
    "\n",
    "If we have a confidence interval for a single target parameter, say the population mean (mu), then we can convert this interval into a hypothesis test by declaring that the null hypothesis mu=c is rejected if c is not inside the confidence interval. Here c is a specific numerical value that must be pre-specified. The type I error rate of the resulting test is 1 minus the coverage probability of the confidence interval. Thus, a 95% coverage probability interval is equivalent in this sense to a hypothesis test with 5% type I error rate (i.e. ɑ = 0.05). There are variations on this equivalence for one-sided tests, which are equivalent to one-sided intervals, and for two-sided tests, which are equivalent to two-sided intervals.\n",
    "\n",
    "Going the other way, if we have a hypothesis testing procedure for a parameter mu, then we can conduct a test of the null hypothesis mu=c for all possible values of c, at a fixed type I error rate. The set of all values of c for which the null hypothesis is not rejected is a confidence interval. If the tests are conducted with type I error rate alpha (e.g. ɑ=0.05), then the coverage probability of the resulting interval is 1 - ɑ (e.g. 0.95). This process is often called “inverting a hypothesis test to construct a confidence set”. Note that in practice it is not actually possible to conduct the test for all possible values of c, because there are infinitely many such values. There are numerical algorithms to address this computational issue, but we will not cover them here.\n",
    "\n",
    "More subtle questions arise when we are working with two or more parameters. For example, suppose we have samples of female and male office workers, and are comparing their incomes. We can construct two 95% confidence intervals -- one for the women and one for the men. It is natural to think that if these two intervals do not overlap, then we can reject the two-sample null hypothesis that the population means are the same, and conversely, if the two intervals do overlap, then we do not reject the null hypothesis that the population means are the same. But in fact, only one of these assertions is true. When the confidence intervals do not overlap, then the two-sample test will always reject its null hypothesis. But the converse is actually not true. There are situations where the confidence intervals overlap, but the hypothesis test will still reject its null hypothesis.\n",
    "\n",
    "In general, it is safer to conduct a hypothesis test explicitly if that is the type of result that will be reported. But if for some reason it is not possible to conduct the hypothesis test, but the intervals are available (e.g. they appear in a paper but the primary data are not accessible), then it is important to remember that the test results cannot always be inferred from knowing only whether the confidence intervals overlap."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NTS\n",
    "QUESTION 1 / 10\n",
    "Carol's fruit stand plans to perform an investigation into how to best preserve its top-selling item, grapes. Their study will involve taking 200 grapes, wrapping 95 of them in a special plastic wrap, and leaving the remaining grapes unwrapped. After being stored for a week, each grade will be examined to see if it has spoiled and the incidence of spoilage for the two packaging methods will be compared.\n",
    "Correct, the answer is Two Proportions (Independent).\n",
    "\n",
    "There is one categorical variable, whether the grape is spoiled. Grapes wrapped in plastic and unwrapped grapes can be thought of as two separate populations here. They can test for a difference in the two population proportions to see if wrapping the grapes affects spoilage rates. \n",
    "\n",
    "<br>\n",
    "QUESTION 2 / 10\n",
    "Many of the students at your university think that you are awesome! You would even wager that the majority of students think so. After a poll is sent via email in which your statistics classmates answer whether they think you're awesome, you perform the appropriate procedure to test your hypothesis.\n",
    "\n",
    "Correct, the answer is One Proportion.\n",
    "\n",
    "Your population of interest is all students at your university. Since your sample is asked a yes/no question, you are dealing with a proportion. You conduct a hypothesis test to see if the true population proportion of students at your university who think you're awesome is greater than .5. Awesome!\n",
    "\n",
    "<br>\n",
    "QUESTION 7 / 10\n",
    "About 10% of all monkeys do not like bananas. Suppose that a researcher speculates that monkeys at the Bronx Zoo are more likely to enjoy bananas than other monkeys. The researcher offers monkeys in one of the many habitats at the Bronx Zoo bananas and records whether they eat the bananas or reject them.\n",
    "\n",
    "Correct, the answer is One Proportion.\n",
    "\n",
    "Our population of interest here is all monkeys at the Bronx Zoo (careful - not all monkeys!) Since liking bananas can be thought of as having a yes/no response, the researcher is dealing with a proportion. He want to know if monkeys at the Bronx Zoo are more likely to enjoy bananas than monkeys in general, and 90% (100%-10% that do not like bananas) of monkeys like bananas, so he should perform a hypothesis test to see if the true population proportion of Bronx Zoo monkeys that like bananas is greater than .9.\n",
    "\n",
    "<br>\n",
    "QUESTION 9 / 10\n",
    "A cafeteria lunch lady wants to know what proportion of the students at her school like her new macaroni and cheese. Students at this school are divided into 4 lunch groups. As the first group of students leaves the cafeteria, they are asked if they liked the new dish.\n",
    "\n",
    "Correct, the answer is One Proportion.\n",
    "\n",
    "We have one population, all students at the school, and we know from the problem that we are dealing with a proportion (we also know that students will be giving a yes/no response when they leave the cafeteria, which also indicates that we are working with a proportion). We could create a confidence interval to find the true population proportion of students who like the new macaroni and cheese.\n",
    "\n",
    "<br>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
